{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d909654-b536-4dca-9561-2ba2920c5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282953e-e66b-4943-8388-7dfc7177658c",
   "metadata": {},
   "source": [
    "### **ATTENTION:**\n",
    "- Watch out for **overfitting**, which happens when a neural network essentially “memorizes” the training data. Overfitting means you get great performance on training data, but the network’s model is useless for out-of-sample prediction.\n",
    "- Regularization helps: regularization methods include l1, l2, and dropout among others.\n",
    "- So have a separate test set on which the network doesn’t train.\n",
    "- The larger the network, the more powerful, but it’s also easier to overfit. Don’t want to try to learn a million parameters from 10,000 examples – parameters > examples = trouble.\n",
    "- **More data** is almost always better, because it helps fight overfitting.\n",
    "- Train over multiple epochs (complete passes through the dataset).\n",
    "- Evaluate test set performance at each epoch to know when to stop (**early stopping**).\n",
    "- In general, stacking layers can help.\n",
    "- For LSTMs, use the softsign (not softmax) activation function over tanh (it’s faster and less prone to saturation (~0 gradients)).\n",
    "- Updaters: RMSProp, AdaGrad or momentum (Nesterovs) are usually good choices. AdaGrad also decays the learning rate, which can help sometimes.\n",
    "- Finally, remember **data normalization**, MSE loss function + identity activation function for regression, Xavier weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026f39ce-6dd6-4c83-b7ee-8474e7251cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.set_printoptions(precision=6,sci_mode=False)\n",
    "pd.set_option('display.float_format',lambda x : '%.6f' % x)\n",
    "\n",
    "SEED = 2024\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdd660-d91e-497a-b131-e9752e06c961",
   "metadata": {},
   "source": [
    "### **Step1: Dataset Load**\n",
    "\n",
    "As our prediction is based on each type of squence, we need to generate batches contain different `Outcomes` in one `Reference` sequence. Consequently, we apply class `BatchSampler` and `SubsetRandomSampler` to generate batches and then pack them to `gRNADataset`.\n",
    "\n",
    "[torch.utils.data](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "`class torch.utils.data.Sampler(data_source=None)`: Base class for all Samplers. Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and may provide a __len__() method that returns the length of the returned iterators.\n",
    "\n",
    "`DataLoader`(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False)\n",
    "\n",
    "**Parameters**: data_source (Dataset) – This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.\n",
    "\n",
    "\n",
    "**class definition:**\n",
    "```python\n",
    "class Book:\n",
    "    def __init__(self, title, author):\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.pages = []\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.pages)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pages)\n",
    "        \n",
    "    def add_pages(self, content):\n",
    "        self.pages.append(content)\n",
    "\n",
    "my_book = Book('python', 'Martina')\n",
    "my_book.add_pages('Chapter 1')\n",
    "my_book.add_pages('Chapter 2')\n",
    "\n",
    "for page in my_book:\n",
    "    print(page)\n",
    "\n",
    "print(my_book, len(my_book))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa97e88-8467-497b-844a-a597c275e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(Sampler):\n",
    "    def __init__(self, data):\n",
    "        idx_list = []\n",
    "        for ref, sub_df in data.groupby('Reference'):\n",
    "            idx_list.append(sub_df.index.tolist())\n",
    "        self.sampler = SubsetRandomSampler(idx_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx in self.sampler:\n",
    "            yield idx\n",
    "\n",
    "class gRNADataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.indices = self.df['index'].tolist()\n",
    "        self.offsets = self.df['offsets'].tolist()\n",
    "        self.cnt = self.df['Count'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx]\n",
    "        offsets = self.offsets[idx]\n",
    "        cnt = self.cnt[idx]\n",
    "        return indices, offsets, cnt\n",
    "\n",
    "class testDataset:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.indices = self.df['index'].tolist()\n",
    "        self.offsets = self.df['offsets'].tolist()\n",
    "        self.cnt = self.df['True_Proportion'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.indices[idx]\n",
    "        offsets = self.offsets[idx]\n",
    "        cnt = self.cnt[idx]\n",
    "        return indices, offsets, cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5551c77-5d13-490d-affd-b0d74506aa6a",
   "metadata": {},
   "source": [
    "### **Step2: Generate Encoding and Batches**\n",
    "\n",
    "1. `generate_encoding`: use number `0` to `3` to represent bases and iterates through both sequences simultaneously, encoding each pair of nucleotides. Then it creates an offset list for every even position in the encoded sequence.\n",
    "2. `generate_batch`: process a batch of datas for input into our model. Firstly flatten the encoded sequences, from all items in the batch into a single list. Then adjust offsets for each items in one batch and put all of `offsets` and `indices` together to a single list.\n",
    "\n",
    "```python\n",
    "def generate_encoding(ref, out):\n",
    "    STOI = str.maketrans('ACGT', '0123') ## string to integer\n",
    "    idx = [int(nuc) for pair in zip(ref.translate(STOI), out.translate(STOI)) for nuc in pair]\n",
    "    ofs = list(range(0, len(idx), 2))\n",
    "    return idx, ofs\n",
    "\n",
    "idx, ofs = generate_encoding('TTTAACCCGG', 'TTTCCCTTTAA')\n",
    "print(idx,ofs)\n",
    "idx = [3, 3, 3, 3, 3, 3, 0, 1, 0, 1, 1, 1, 1, 3, 1, 3, 2, 3, 2, 0]\n",
    "ofs = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "\n",
    "def generate_batch(batch):\n",
    "    indices = [idx for item in batch for idx in item[0]]\n",
    "    offsets = [ofs + i * len(item[0]) for i, item in enumerate(batch) for ofs in item[1]]\n",
    "    cnts = [item[2] for item in batch]\n",
    "    return indices, offsets, cnts\n",
    "```\n",
    "**Sample Usage:**\n",
    "\n",
    "```python\n",
    "sample_data = [(\"ACGTACGTGTCCTGGCTGCC\", \"ACGTGCGTGTCCTGGCTGCC\", 10), (\"CATATCTACGCGTCGCACTT\", \"CATGTCTACGCGTCGCACTT\", 20)]\n",
    "\n",
    "df = pd.DataFrame(sample_data, columns = ['Reference', 'Outcomes', 'Count'])\n",
    "df['index'] = None\n",
    "df['offsets'] = None\n",
    "df[['index', 'offsets']] = df.apply(lambda x: generate_encoding(x['Reference'], x['Outcomes']), axis=1, result_type='expand')\n",
    "batch = list(zip(df['index'], df['offsets'], df['Count']))\n",
    "indices, offsets, cnts = generate_batch(batch)\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "         Reference              Outcomes  Count index offsets\n",
    "0  ACGTACGTGTCCTGGCTGCC  ACGTGCGTGTCCTGGCTGCC     10  None    None\n",
    "1  CATATCTACGCGTCGCACTT  CATGTCTACGCGTCGCACTT     20  None    None\n",
    "              Reference              Outcomes  Count  \\\n",
    "0  ACGTACGTGTCCTGGCTGCC  ACGTGCGTGTCCTGGCTGCC     10   \n",
    "1  CATATCTACGCGTCGCACTT  CATGTCTACGCGTCGCACTT     20   \n",
    "\n",
    "                                               index  \\\n",
    "0  [0, 0, 1, 1, 2, 2, 3, 3, 0, 2, 1, 1, 2, 2, 3, ...   \n",
    "1  [1, 1, 0, 0, 3, 3, 0, 2, 3, 3, 1, 1, 3, 3, 0, ...   \n",
    "\n",
    "                                             offsets  \n",
    "0  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24...  \n",
    "1  [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24...  \n",
    "indices:\n",
    "tensor([0, 0, 1, 1, 2, 2, 3, 3, 0, 2, 1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 1, 1, 1, 1,\n",
    "        3, 3, 2, 2, 2, 2, 1, 1, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 3, 3, 0, 2,\n",
    "        3, 3, 1, 1, 3, 3, 0, 0, 1, 1, 2, 2, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 1, 1,\n",
    "        0, 0, 1, 1, 3, 3, 3, 3]) \n",
    " offsets:\n",
    "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
    "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n",
    "        72, 74, 76, 78]) \n",
    " cnts:\n",
    "tensor([10., 20.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248bffb6-9340-496e-bb27-6cc3d08410b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoding(ref, out):\n",
    "    STOI = str.maketrans('ACGT', '0123')\n",
    "    idx = [int(nuc) for pair in zip(ref.translate(STOI), out.translate(STOI)) for nuc in pair]\n",
    "    ofs = list(range(0, len(idx), 2))\n",
    "    return idx, ofs\n",
    "\n",
    "def generate_batch(batch):\n",
    "    indices = [idx for item in batch for idx in item[0]]\n",
    "    offsets = [ofs + i * len(item[0]) for i, item in enumerate(batch) for ofs in item[1]]\n",
    "    counts = [item[2] for item in batch]\n",
    "    ref = [item[0] for item in batch]\n",
    "    otm = [item[1] for item in batch]\n",
    "    \n",
    "    return torch.LongTensor(indices), torch.LongTensor(offsets), torch.FloatTensor(counts), torch.LongTensor(ref), torch.LongTensor(otm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b04bce5-93f1-455d-9c47-699dcc901a91",
   "metadata": {},
   "source": [
    "## Formulas\n",
    "\n",
    "- $f$: fully connected layer\n",
    "- $\\mathbf{h}$: hidden state\n",
    "- $s$: output scores\n",
    "- $k$: the total number of certain reference's outcomes\n",
    "\n",
    "**Step1: From biLSTM get output context vector**\n",
    "\n",
    "$\\mathbf{h}$ represent the hidden state, and for the last hidden state, we optimize it as:\n",
    "\n",
    "$$\n",
    "Output\\ Score_{(s)} = LeakReLU(f(\\mathbf{h_last}))\n",
    "$$\n",
    "\n",
    "**Step2: Generate Predicted Outcomes**\n",
    "\n",
    "And we can denote a batch of output score as: $Output\\ Scores\\ Set = [s_1,s_2,s_3 ...s_i...s_k], i \\in \\mathbf{R}^k$. Then, we put $softmax$ function into $s$ to generate the prediction:\n",
    "$$\n",
    "pred_i = softmax(\\mathbf{s}) = \\frac{e^{s_i}}{\\sum_{j = 1}^{k}e^{s_j}}\n",
    "$$\n",
    "$$\n",
    "pred_{all} = q = \\sum_{j = 1}^{k}q_j = 1\n",
    "$$\n",
    "\n",
    "**Step3: True Scores**\n",
    "\n",
    "Here, we regard $true_i$ as conresponded true score, and the set is denoted as $p =[p_1,p_2,p_3 ... p_j... p_{k}], j \\in \\mathbf{R}^{k}$\n",
    "\n",
    "**Step4: Loss Function**\n",
    "\n",
    "To measure the distance between $pred_i$ and $true_j$, we put method $\\mathbf{KL-Divergence}$ and add the weight $w_i$ of each outcomes of each single batch, which is counted by:\n",
    "$$\n",
    "\\frac{c_i}{\\sum_{i=1}^{k}c_i}, i \\in \\mathbf{R}^k\n",
    "$$\n",
    "$c_i$ is the true count number of the certain true outcomes, and then we calculate the **loss1** as $L_1$:\n",
    "$$\n",
    "L_1 = \\mathbf{D}_{KL}(q\\Arrowvert{p}) = \\sum_{i = 1}^{k}w_i p_i log(\\frac{p_i}{q_i})\n",
    "$$\n",
    "Besides, considering the mean squared error (MSE) can also shrink the difference between $pred_i$ and $pred_i$, we also put it as **loss2** as $L_2$:\n",
    "$$\n",
    "L_2 = \\frac{1}{2K} \\sum_{i=1}^{k} w_i (p_i - q_i)^2\n",
    "$$\n",
    "Finally, the total **Loss** is:\n",
    "$$\n",
    "L = \\frac{1}{2}L_1 + \\frac{1}{2}L_2\n",
    "$$\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d323a9-f123-4188-9794-7781a3eb1ab6",
   "metadata": {},
   "source": [
    "### **Step3: BiLSTM with Attention Model**\n",
    "<center><img class=\"image\" id=\"me\" src=\"attachment:0f4eabb1-1ec4-40c5-a73a-3b3b4cfec21e.png\" width = \"500\"></center>\n",
    "\n",
    "1. `EmbeddingBag`:embedding layer, to pool the embeddings of variable-length sequences. This pooling method reduces them to fixed-size vectors.\n",
    "2. `nn.LSTM`: Bidirectional LSTM layer, which learns temporal dependencies in both forward and backward directions.\n",
    "3. `attention_net`: computes the attention scores for a query over a squence. Calculates the dot-product attention between query and sequence x. `scores`, it's attention scores matrix that measures relevance between each element of the sequence and the query. `alpha_n`, attention weights, computed by applying softmax to the scores. `context`, weighted sum of the sequence x, according to the attention weights, which summarizes the sequence.\n",
    "4. `fc`: fully connected layer that produces output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60312538-ad7e-402f-a870-6e8f9b4d83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    '''\n",
    "    Optimized: to prevent early-stage vanishing gradients in LSTMs, can let model learn long-term dependencies better.\n",
    "    '''\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'rnn.weight_' in name:\n",
    "            nn.init.orthogonal_(param.data) ## orthogonal initialization of LSTM weights\n",
    "        elif 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01) ## Xavier initialization for non-recurrent weights\n",
    "        elif 'bias' in name:\n",
    "            if 'rnn.bias_' in name:\n",
    "                nn.init.constant_(param.data, 1) ## set the forget gate bias in LSTM to 1\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0) ## set other biases to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc0a305-4d45-444d-bfa4-4a69f0f8455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40bf00b5-d99d-4e91-bf86-6d90110d0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c40191-d63a-47d3-8b3e-6123f24242a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_Attention(my_input_dim, my_emb_dim, my_hid_dim, my_layers, my_dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0736332-aa85-4bf4-af52-88324e6139ae",
   "metadata": {},
   "source": [
    "### **Step4: Define Training and Testing Function**\n",
    "\n",
    "#### **4.1 Choose which one of `PyTorch loss functions`:**\n",
    "\n",
    "For [nn.MSELoss:](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) ---> 3 params: none, mean, sum\n",
    "\n",
    "`reduction (str, optional)` – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n",
    "\n",
    "1.\t`none: criterion = nn.MSELoss(reduction='none')`:\n",
    "    - Behavior: No reduction is applied. This means the loss is computed element-wise and returned as a tensor with the same shape as the input.\n",
    "    - Use Case: Useful when you need the loss for each element separately, e.g., for per-sample loss tracking or custom aggregation.\n",
    "2.\t`mean: criterion = nn.MSELoss(reduction='mean')`:\n",
    "    - Behavior: The loss is averaged across all elements of the input tensor.\n",
    "    - Use Case: This is the most common setting, as it gives a single scalar representing the average loss across all elements. Use this if you want the average loss per element.\n",
    "3.\t`sum: criterion = nn.MSELoss(reduction='sum')`:\n",
    "    - Behavior: The loss is summed across all elements of the input tensor.\n",
    "    - Use Case: Useful when you want the total loss across all elements, rather than an average. Typically used when you need the overall magnitude of the loss or when dealing with batch loss where the total matters (e.g., when normalizing by the total number of elements later).\n",
    "\n",
    "- `none`: No reduction; per-element loss.\n",
    "- `mean`: Average loss per element (most common).\n",
    "- `sum` Sum of losses across all elements (useful if you need the total loss or will normalize by batch size later).\n",
    "\n",
    "For [nn.KLDivLoss:](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) ---> 4 params: none, mean, batch mean, sum\n",
    "\n",
    "This function is used to measure the divergence between two **probability distributions**. Since KLD loss often deals with log probabilities, it’s common to use 'batchmean' or 'sum' to compute the overall divergence for a batch.\n",
    "Common choice:\n",
    "\n",
    "```python\n",
    "if reduction == \"mean\":  # default\n",
    "    loss = loss_pointwise.mean()\n",
    "elif reduction == \"batchmean\":  # mathematically correct\n",
    "    loss = loss_pointwise.sum() / input.size(0)\n",
    "elif reduction == \"sum\":\n",
    "    loss = loss_pointwise.sum()\n",
    "else:  # reduction == \"none\"\n",
    "    loss = loss_pointwise\n",
    "```\n",
    "\n",
    "Here we choose both `none`, which means each of the elements in one batch is calculated seperately? This will print a tensor of the same shape as the input, where each element represents the KLD loss for that particular element. Becuase we need to perform some custom reduction later (e.g., **weighting each element’s loss differently**, or computing per-sample loss), then `reduction='none'` is useful.\n",
    "\n",
    "****\n",
    "\n",
    "- `softmax`: result from 0 to 1; `logsoftmax`: from - $\\infty$ to 0\n",
    "\n",
    "****\n",
    "\n",
    "#### **4.2 Debug: The output dimensions of each line**\n",
    "\n",
    "As what we mentioned before, the output is like:\n",
    "```\n",
    "y_pred_1 --->: (softmax from outputs)   \n",
    "tensor([0.032258, 0.032229, 0.032282, 0.032281, 0.032286, 0.032240, 0.032271,\n",
    "        0.032273, 0.032271, 0.032263, 0.032207, 0.032257, 0.032261, 0.032250,\n",
    "        0.032254, 0.032273, 0.032274, 0.032233, 0.032268, 0.032244, 0.032247,\n",
    "        0.032256, 0.032227, 0.032283, 0.032283, 0.032243, 0.032284, 0.032250,\n",
    "        0.032269, 0.032272, 0.032213], device='cuda:0', grad_fn=<ViewBackward>)\n",
    "y_pred_2: (logsoftmax from outputs)\n",
    "tensor([-3.433977, -3.434890, -3.433241, -3.433290, -3.433125, -3.434560,\n",
    "        -3.433588, -3.433515, -3.433597, -3.433846, -3.435585, -3.434012,\n",
    "        -3.433898, -3.434241, -3.434123, -3.433510, -3.433484, -3.434758,\n",
    "        -3.433667, -3.434437, -3.434321, -3.434052, -3.434946, -3.433228,\n",
    "        -3.433214, -3.434468, -3.433172, -3.434247, -3.433659, -3.433563,\n",
    "        -3.435398], device='cuda:0', grad_fn=<ViewBackward>)\n",
    "y_true:\n",
    "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
    "```\n",
    "\n",
    "The problem is that outputs are quite similar, so it means the model **an not perform well**. Then I find it's because of the `weight`, after I add it:\n",
    "```\n",
    "y predicted 1: tensor([-1.939422, -1.944494, -1.945731, -1.946127, -1.946525, -1.947677,\n",
    "        -1.951434], device='cuda:0', grad_fn=<ViewBackward>)\n",
    "y predicted 2: tensor([0.143787, 0.143060, 0.142883, 0.142826, 0.142769, 0.142605, 0.142070],\n",
    "       device='cuda:0', grad_fn=<ViewBackward>)\n",
    "outputs:tensor([[-0.000286],\n",
    "        [-0.005358],\n",
    "        [-0.006595],\n",
    "        [-0.006991],\n",
    "        [-0.007389],\n",
    "        [-0.008541],\n",
    "        [-0.012298]], device='cuda:0', grad_fn=<LeakyReluBackward0>)\n",
    "true scores: tensor([0.007529, 0.019860, 0.360981, 0.011812, 0.049844, 0.087098, 0.462876], ...\n",
    "loss 1 KLD: tensor([0.005233, 0.013803, 0.250890, 0.008210, 0.034643, 0.060535, 0.321709], ...\n",
    "loss 2 MSE: tensor([0.000229, 0.000605, 0.010990, 0.000360, 0.001517, 0.002652, 0.014092], ...\n",
    "```\n",
    "The loss performs much better now. But still, the predicted outputs are quite the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f559fad-3931-4ce9-9989-2bd0113370b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, clips, fold, trained_model_path, train_dataloader, valid_dataloader, criterion_KLD, criterion_mse, device, basename, optimizer):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    \n",
    "    # avg_valid_losses = [] \n",
    "    best_valid_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_per_epoch = len(train_dataloader)\n",
    "        kbar = pkbar.Kbar(target=train_per_epoch, epoch=epoch, num_epochs=epochs, width=8, always_stateful=False)\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            indices, offsets, counts, ref, otm = batch\n",
    "            indices = indices.to(device)\n",
    "            offsets = offsets.to(device)\n",
    "            counts = counts.to(device)\n",
    "            all_counts = counts.sum() + 1e-6\n",
    "            y_true = counts / all_counts\n",
    "            batch_size = int(len(indices) / 40)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(indices, offsets, batch_size)\n",
    "            \n",
    "            y_pred_1 = F.log_softmax(outputs, 0).view(-1)\n",
    "            loss_temp_1 = criterion_KLD(y_pred_1, y_true)\n",
    "            loss_temp_1 = torch.abs(loss_temp_1 * y_true) * 100\n",
    "            \n",
    "            y_pred_2 = F.softmax(outputs, 0).view(-1)\n",
    "            loss_temp_2 = criterion_mse(y_pred_2, y_true)\n",
    "            loss_temp_2 = loss_temp_2 * y_true * 100\n",
    "            \n",
    "            print(f'outputs:\\n{outputs}\\npred_y_1: \\n{y_pred_1}\\npred_y_1.shape:\\n{y_pred_1.shape}\\npred_y_2: \\n{y_pred_2}\\npred_y_2.shape:\\n{y_pred_2.shape}\\ny_true: {y_true}\\n y_true.shape:\\n {y_true.shape}')\n",
    "\n",
    "            loss1 = loss_temp_1.mean()\n",
    "            loss2 = loss_temp_2.mean()\n",
    "            \n",
    "            loss = 0.3 * loss1 + 0.7 * loss2\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            kbar.update(i, values=[(\"training loss\", train_loss/(i + 1)),(\"loss1\",loss1),(\"loss2\",loss2),(\"lr\", optimizer.defaults['lr'])])\n",
    "            \n",
    "        train_loss_ = train_loss / len(train_dataloader)\n",
    "        train_loss_list.append(train_loss_)\n",
    "\n",
    "        \n",
    "        valid_loss = test_model(valid_dataloader, model, criterion_KLD, criterion_mse, device)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f'{trained_model_path}/{basename}_{fold+1}.pth')\n",
    "            print(f\"New best model saved for fold {fold+1} with validation loss {valid_loss:.6f}\")\n",
    "\n",
    "        ## logging\n",
    "        print(f'Fold {fold + 1}, Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss_:.6f}, Validation Loss: {valid_loss:.6f}')\n",
    "        \n",
    "        scheduler.step(valid_loss) \n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Learning Rate: {param_group['lr']}\")\n",
    "    return train_loss_list, valid_loss_list\n",
    "\n",
    "def test_model(test_dataloader, model, criterion_KLD, criterion_mse, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    lst_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            indices, offsets, counts, ref, otm = batch\n",
    "            indices = indices.to(device)\n",
    "            offsets = offsets.to(device)\n",
    "            counts = counts.to(device)\n",
    "            all_counts = counts.sum() + 1e-6\n",
    "            y_true = counts / all_counts\n",
    "            batch_size = int(len(indices) / 40)            \n",
    "\n",
    "            outputs = model(indices, offsets, batch_size)\n",
    "            outs = outputs.view(-1)\n",
    "            df_on = pd.DataFrame({'Reference': ref,'Outcomes': otm})\n",
    "            df_on['true'] = y_true.cpu().detach().numpy()\n",
    "            df_on['pred'] = outs.cpu().detach().numpy()\n",
    "            lst_pred.append(df_on)\n",
    "            \n",
    "            pred_y_1 = F.log_softmax(outputs, dim=0).view(-1)\n",
    "            loss_temp_1 = criterion_KLD(pred_y_1, y_true)\n",
    "            loss_temp_1 = torch.abs(loss_temp_1 * y_true)\n",
    "            pred_y_2 = F.softmax(outputs, dim=0).view(-1)\n",
    "            loss_temp_2 = criterion_mse(pred_y_2, y_true)\n",
    "            loss_temp_2 = loss_temp_2 * y_true\n",
    "            \n",
    "            loss1 = loss_temp_1.mean()\n",
    "            loss2 = loss_temp_2.mean()\n",
    "            loss = loss1 + loss2\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= len(test_dataloader)\n",
    "    df = pd.concat(lst_pred)\n",
    "    return test_loss,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10dfb165-c23a-46de-8b03-e49345e7dcc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset, list_gRNA, grp_df = process_data(train_data)\n",
    "test_dataset, list_gRNA_test, grp_test = process_data_test(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86efe855-a791-4434-9b90-95ef438f5ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion_KLD = nn.KLDivLoss(reduction='none')\n",
    "criterion_mse = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d05bce45-dd26-414b-b04a-6f8ed963a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47c4b4e-2f36-4285-b148-bedc38a1cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ff74e-f782-41bc-9a32-c861d2351711",
   "metadata": {},
   "source": [
    "### **Step5.Testing and Evaluate Performances**\n",
    "****\n",
    "\n",
    "**Tensor Score Similar Problem:**\n",
    "\n",
    "Finally, I changed the model as `BiLSTM` without `Attention Mechanism`, and still I use `nn.LSTM` instead of `GRU`. Additionally, I used `hid_dim = 256` and `n_layer = 2`. Then I observed that as the training progress going on, the `outputs` be more reasonable for it \"learned\" the information. **You can scroll down the outputs below to look for the changes.**\n",
    "\n",
    "Next, I plan to draw plots of correlation coefficience for each test data of each type of base editor and also there are some of statistical results need to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3cdfd0-da78-40eb-aec7-aa81d9592b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_plot(fold_list, epoch_list, train_list, valid_list, file_path, basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c01e1e-9b33-487b-96b9-b7636b0b8eeb",
   "metadata": {},
   "source": [
    "### **Step6: Tuning Suggestion:**\n",
    "\n",
    "1.\tBatch handling: Ensuring that the reference sequences are correctly handled for each batch. Since you mention the reference sequence remains constant per batch, let’s confirm the batching process handles it correctly.\n",
    "    - However, from the predicted values, it seems that **all predictions are very similar**, which could indicate that the model isn’t learning to distinguish between sequences effectively. This could be due to several factors like **gradient vanishing**, **improper scaling of the input**, or **issues in how the batch is constructed**.\n",
    "    - Since your model takes reference sequences and outcomes (for both ABE and CBE), it’s crucial that these sequences are well balanced within each batch. However, looking at the predictions:\n",
    "    - **y_pred_1 and y_pred_2** show very similar values across the batch. This uniformity could indicate that the input sequences in the batch may be too similar or that there is an issue in how the batch data is processed and fed into the model.\n",
    "\n",
    "3.\tLoss: You are using two loss functions:\n",
    "    - **KLD (Kullback-Leibler Divergence)**: The KLD loss between the predicted y_pred_1 and the true target might not be changing much because all predictions are very close in magnitude (-3.0916). This could imply that the model is stuck in a local minimum or the gradients are not properly backpropagating.\n",
    "    - **MSE (Mean Squared Error)**: The MSE loss on the second set of predictions (y_pred_2) is very small, which could indicate that the model is not being penalized strongly enough, potentially leading to little learning in early epochs.\n",
    "4.\tModel initialization and learning rate: An unstable loss at the beginning could also stem from improper initialization or learning rate settings.\n",
    "5.\tAttention mechanism integration: Since the model combines LSTM and attention, proper integration and normalization of attention weights could affect early training stability.\n",
    "6.\tParameters fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97970935-c495-4b33-a3a5-2541b4db5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor1.15.0",
   "language": "python",
   "name": "tensor1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
